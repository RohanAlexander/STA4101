---
title: "Demonstration"
author: "Rohan Alexander"
date: "2025-09-22"
date-format: "D MMMM, YYYY"
format: pdf
bibliography: references.bib
---

# Generating synthetic datasets

# Make effective graphs and tables

Manipulate the data into a summary table.

```{python}
#| eval: false
#| echo: true
#| warning: false
#| message: false

import polars as pl

df = pl.read_parquet("cleaned_shelter_usage.parquet")

# Convert the date column to datetime and rename it for clarity
df = df.with_columns(pl.col("date").str.strptime(pl.Date, "%Y-%m-%d").alias("date"))

# Group by "Dates" and calculate total "Capacity" and "Usage"
aggregated_df = (
    df.group_by("date")
    .agg([
        pl.col("Capacity").sum().alias("Total_Capacity"),
        pl.col("Usage").sum().alias("Total_Usage")
    ])
    .sort("date")  # Sort the results by date
)

# Display the aggregated DataFrame
print(aggregated_df)
```


# Fitting ridge, lasso, logistic, Poisson, and random forests with scikit-learn

```{python}
#| eval: false
#| echo: true

uv add scikit-learn polars seaborn pandas
```

```{python}
#| eval: false
#| echo: true

# titanic_sklearn_polars.py
import seaborn as sns
import polars as pl
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# 1) Load → polars
titanic_pd = sns.load_dataset("titanic")  # returns pandas
df = pl.from_pandas(titanic_pd)

# Keep a small, robust subset of columns
cols = ["survived","pclass","sex","age","sibsp","parch","fare","embarked"]
df = df.select(cols)

# Basic cleaning with polars (drop rows missing in key fields we’ll use)
df = df.drop_nulls(subset=["pclass","sex","embarked"])  # keep target-specific handling later

# Back to pandas for sklearn
df_pd = df.to_pandas()

# Common feature sets
num_features = ["pclass","age","sibsp","parch","fare"]
cat_features = ["sex","embarked"]

# Preprocessor: impute + one-hot
preprocessor = ColumnTransformer(
    transformers=[
        ("num", SimpleImputer(strategy="median"), num_features),
        ("cat", Pipeline([
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("ohe", OneHotEncoder(handle_unknown="ignore"))
        ]), cat_features),
    ]
)
```

```{python}
#| eval: false
#| echo: true


```



```{python}
#| eval: false
#| echo: true


```


```{python}
#| eval: false
#| echo: true


```

# Implementing bootstrap and cross-validation

# Quarto docs

